{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[ 0.,  0.,  5., ...,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 10.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., ..., 16.,  9.,  0.],\n",
       "        ...,\n",
       "        [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "        [ 0.,  0.,  2., ..., 12.,  0.,  0.],\n",
       "        [ 0.,  0., 10., ..., 12.,  1.,  0.]]),\n",
       " 'target': array([0, 1, 2, ..., 8, 9, 8]),\n",
       " 'frame': None,\n",
       " 'feature_names': ['pixel_0_0',\n",
       "  'pixel_0_1',\n",
       "  'pixel_0_2',\n",
       "  'pixel_0_3',\n",
       "  'pixel_0_4',\n",
       "  'pixel_0_5',\n",
       "  'pixel_0_6',\n",
       "  'pixel_0_7',\n",
       "  'pixel_1_0',\n",
       "  'pixel_1_1',\n",
       "  'pixel_1_2',\n",
       "  'pixel_1_3',\n",
       "  'pixel_1_4',\n",
       "  'pixel_1_5',\n",
       "  'pixel_1_6',\n",
       "  'pixel_1_7',\n",
       "  'pixel_2_0',\n",
       "  'pixel_2_1',\n",
       "  'pixel_2_2',\n",
       "  'pixel_2_3',\n",
       "  'pixel_2_4',\n",
       "  'pixel_2_5',\n",
       "  'pixel_2_6',\n",
       "  'pixel_2_7',\n",
       "  'pixel_3_0',\n",
       "  'pixel_3_1',\n",
       "  'pixel_3_2',\n",
       "  'pixel_3_3',\n",
       "  'pixel_3_4',\n",
       "  'pixel_3_5',\n",
       "  'pixel_3_6',\n",
       "  'pixel_3_7',\n",
       "  'pixel_4_0',\n",
       "  'pixel_4_1',\n",
       "  'pixel_4_2',\n",
       "  'pixel_4_3',\n",
       "  'pixel_4_4',\n",
       "  'pixel_4_5',\n",
       "  'pixel_4_6',\n",
       "  'pixel_4_7',\n",
       "  'pixel_5_0',\n",
       "  'pixel_5_1',\n",
       "  'pixel_5_2',\n",
       "  'pixel_5_3',\n",
       "  'pixel_5_4',\n",
       "  'pixel_5_5',\n",
       "  'pixel_5_6',\n",
       "  'pixel_5_7',\n",
       "  'pixel_6_0',\n",
       "  'pixel_6_1',\n",
       "  'pixel_6_2',\n",
       "  'pixel_6_3',\n",
       "  'pixel_6_4',\n",
       "  'pixel_6_5',\n",
       "  'pixel_6_6',\n",
       "  'pixel_6_7',\n",
       "  'pixel_7_0',\n",
       "  'pixel_7_1',\n",
       "  'pixel_7_2',\n",
       "  'pixel_7_3',\n",
       "  'pixel_7_4',\n",
       "  'pixel_7_5',\n",
       "  'pixel_7_6',\n",
       "  'pixel_7_7'],\n",
       " 'target_names': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       " 'images': array([[[ 0.,  0.,  5., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ..., 15.,  5.,  0.],\n",
       "         [ 0.,  3., 15., ..., 11.,  8.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 11., ..., 12.,  7.,  0.],\n",
       "         [ 0.,  2., 14., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  6., ...,  0.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ...,  5.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ...,  9.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ...,  6.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  1., ...,  6.,  0.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 10.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  0., ..., 12.,  0.,  0.],\n",
       "         [ 0.,  0.,  3., ..., 14.,  0.,  0.],\n",
       "         [ 0.,  0.,  8., ..., 16.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  9., 16., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  3., 13., ..., 11.,  5.,  0.],\n",
       "         [ 0.,  0.,  0., ..., 16.,  9.,  0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 0.,  0.,  1., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 13., ...,  2.,  1.,  0.],\n",
       "         [ 0.,  0., 16., ..., 16.,  5.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0., 16., ..., 15.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 16.,  0.,  0.],\n",
       "         [ 0.,  0.,  2., ...,  6.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0.,  2., ...,  0.,  0.,  0.],\n",
       "         [ 0.,  0., 14., ..., 15.,  1.,  0.],\n",
       "         [ 0.,  4., 16., ..., 16.,  7.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  0.,  0., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  4., ..., 16.,  2.,  0.],\n",
       "         [ 0.,  0.,  5., ..., 12.,  0.,  0.]],\n",
       " \n",
       "        [[ 0.,  0., 10., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  2., 16., ...,  1.,  0.,  0.],\n",
       "         [ 0.,  0., 15., ..., 15.,  0.,  0.],\n",
       "         ...,\n",
       "         [ 0.,  4., 16., ..., 16.,  6.,  0.],\n",
       "         [ 0.,  8., 16., ..., 16.,  8.,  0.],\n",
       "         [ 0.,  1.,  8., ..., 12.,  1.,  0.]]]),\n",
       " 'DESCR': \".. _digits_dataset:\\n\\nOptical recognition of handwritten digits dataset\\n--------------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 1797\\n    :Number of Attributes: 64\\n    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\\n    :Missing Attribute Values: None\\n    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\\n    :Date: July; 1998\\n\\nThis is a copy of the test set of the UCI ML hand-written digits datasets\\nhttps://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\\n\\nThe data set contains images of hand-written digits: 10 classes where\\neach class refers to a digit.\\n\\nPreprocessing programs made available by NIST were used to extract\\nnormalized bitmaps of handwritten digits from a preprinted form. From a\\ntotal of 43 people, 30 contributed to the training set and different 13\\nto the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\\n4x4 and the number of on pixels are counted in each block. This generates\\nan input matrix of 8x8 where each element is an integer in the range\\n0..16. This reduces dimensionality and gives invariance to small\\ndistortions.\\n\\nFor info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\\nT. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\\nL. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\\n1994.\\n\\n.. topic:: References\\n\\n  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\\n    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\\n    Graduate Studies in Science and Engineering, Bogazici University.\\n  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\\n  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\\n    Linear dimensionalityreduction using relevance weighted LDA. School of\\n    Electrical and Electronic Engineering Nanyang Technological University.\\n    2005.\\n  - Claudio Gentile. A New Approximate Maximal Margin Classification\\n    Algorithm. NIPS. 2000.\\n\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist = load_digits()\n",
    "mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(mnist.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 1797, 64, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mnist.data),len(mnist.target),len(mnist.feature_names),len(mnist.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  2., 13., 16., 12.,  0.,  0.,  0.,  0.,  9., 15., 10., 16.,\n",
       "        3.,  0.,  0.,  0.,  5.,  7.,  5., 16.,  3.,  0.,  0.,  0.,  0.,\n",
       "        0., 10., 14.,  0.,  0.,  0.,  0.,  0.,  5., 16.,  7.,  0.,  0.,\n",
       "        0.,  0.,  0., 14., 16.,  1.,  3.,  7.,  1.,  0.,  3., 16., 12.,\n",
       "       10., 16., 11.,  1.,  0.,  0., 13., 16., 13.,  7.,  1.,  0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist[\"data\"][1600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X and y\n",
    "X, y = mnist[\"data\"], mnist['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "some_digits = X[1600]\n",
    "some_digits_image = some_digits.reshape(8, 8)\n",
    "plt.imshow(some_digits_image, cmap = matplotlib.cm.binary, interpolation='nearest')\n",
    "# plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1600]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually split train/test set\n",
    "X_train, X_test, y_train, y_test = X[:1500], X[297:], y[:1500], y[297:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a layer of randomness by randomly permutatig X_train and y_train\n",
    "# np.random(seed=42)\n",
    "shuffle_index = np.random.permutation(1500)\n",
    "X_train, y_train = X_train[shuffle_index], y_train[shuffle_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a binary classifier, capable of distinguishing between just two classes, 2 and not-2. \n",
    "# Let’s create the target vectors for this classification task\n",
    "\n",
    "# y_train_2 = y_train==2\n",
    "# y_test_2 = y_test==2\n",
    "\n",
    "# from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "# sgd_clf.fit(X_train, y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(random_state=42)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "sgd_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 7, 7, 3, 5, 1, 0, 0, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Those are the first 10 predictions of the first 10 arrays in X\n",
    "sgd_clf.predict([i for i in X_test[0:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 7, 7, 3, 5, 1, 0, 0, 2])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It correctly predict the first 10 arrays of the test set\n",
    "y_test[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The prediction score is quite high being accurate almost 97% of cases\n",
    "sgd_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.932, 0.942, 0.938])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s use the cross_val_score() function to evaluate the SGDClassifier model \n",
    "# using K-fold cross-validation, with three folds\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(sgd_clf, X_train, y_train, cv=3, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Results of cross_val looks a bit lower. So chances are the train set was quite a bit more 'predictable'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 1, 5, ..., 0, 4, 9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "y_train_pred = cross_val_predict(sgd_clf, X_train, y_train, cv=3)\n",
    "y_train_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[149,   0,   0,   0,   1,   1,   0,   0,   0,   0],\n",
       "       [  0, 130,   1,   1,   0,   0,   3,   0,  14,   2],\n",
       "       [  0,   0, 148,   2,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0, 147,   0,   1,   0,   0,   2,   3],\n",
       "       [  0,   0,   0,   0, 143,   0,   1,   2,   0,   2],\n",
       "       [  1,   0,   2,   0,   0, 143,   1,   0,   0,   5],\n",
       "       [  0,   1,   0,   0,   1,   1, 148,   0,   0,   0],\n",
       "       [  0,   0,   0,   1,   1,   0,   0, 144,   0,   3],\n",
       "       [  0,   8,   2,  12,   0,   1,   1,   0, 115,   7],\n",
       "       [  0,   1,   0,   3,   0,   1,   0,   2,   3, 139]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the confusion matrix w/y_train_pred\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_train, y_train_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Each row of the confusion metrix represent an actual class (so from 0 to 9, rows=y_train, col=y_train_pred).\n",
    "\n",
    "Second row: \n",
    "   * 146 '1s' (y_train=y_train_pred) were correctly guessed as '1s'(true positive) \n",
    "   * on the col: 14 were wrongly guessed as '1s', while should have been '8s'\n",
    "   * on the row: 2 were guessed as '5s' but were actually '1s'\n",
    "   \n",
    "The lower on the diag = lower sample/more wrongly guessed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in y_train_pred:\n",
    "    if i==1:\n",
    "        cnt = cnt +1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt = 0\n",
    "for i in y_train:\n",
    "    if i==1:\n",
    "        cnt = cnt +1\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(cm, cmap=plt.cm.gray)\n",
    "plt.show()\n",
    "# The darker on the diag = lower sample/more wrongly guessed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.98675497, 0.        , 0.        , 0.        , 0.00662252,\n",
       "        0.00662252, 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.86092715, 0.00662252, 0.00662252, 0.        ,\n",
       "        0.        , 0.01986755, 0.        , 0.09271523, 0.01324503],\n",
       "       [0.        , 0.        , 0.98666667, 0.01333333, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.96078431, 0.        ,\n",
       "        0.00653595, 0.        , 0.        , 0.0130719 , 0.01960784],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.96621622,\n",
       "        0.        , 0.00675676, 0.01351351, 0.        , 0.01351351],\n",
       "       [0.00657895, 0.        , 0.01315789, 0.        , 0.        ,\n",
       "        0.94078947, 0.00657895, 0.        , 0.        , 0.03289474],\n",
       "       [0.        , 0.00662252, 0.        , 0.        , 0.00662252,\n",
       "        0.00662252, 0.98013245, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.00671141, 0.00671141,\n",
       "        0.        , 0.        , 0.96644295, 0.        , 0.02013423],\n",
       "       [0.        , 0.05479452, 0.01369863, 0.08219178, 0.        ,\n",
       "        0.00684932, 0.00684932, 0.        , 0.78767123, 0.04794521],\n",
       "       [0.        , 0.00671141, 0.        , 0.02013423, 0.        ,\n",
       "        0.00671141, 0.        , 0.01342282, 0.02013423, 0.93288591]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot the errors. \n",
    "# Divide each value in the confusion matrix by the number of images in the corresponding class, \n",
    "# so we can compare error rates instead of absolute number of errors \n",
    "# which would make abundant classes look unfairly bad\n",
    "row_sums = cm.sum(axis=1, keepdims=True)\n",
    "norm_cm = cm/row_sums\n",
    "norm_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now let’s fill the diagonal with zeros to keep only the errors, and let’s plot the result:\n",
    "np.fill_diagonal(norm_cm,0)\n",
    "plt.matshow(norm_cm, cmap=plt.cm.gray)\n",
    "plt.show()\n",
    "# The brightest = most mistakes ie. '1s' with '8s' or '5s' with '9s' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Way to improve the classifier:\n",
    " - Gather more data for these digits\n",
    " - Engineer new features like writing an algorithm to count the number of closed loops (ie., 8 has two, 6 has one)\n",
    " - Or preprocess the images (ie., using Scikit-Image, Pillow, or OpenCV) to make some patterns stand out more, such as closed loops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Analyzing individual errors can also be a good way to gain insights on what your classifier is doing and \n",
    "why it is failing, but it is more difficult and time-consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
